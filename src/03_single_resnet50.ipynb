{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Resnet50 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import bcolz\n",
    "import random\n",
    "from importlib import reload\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resnet_model = ResNet50(include_top=False, input_shape=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting last layer vectors for all galaxy images take a lot of time, so lets process them once and save results on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array(fname, arr): \n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "def load_array(fname): \n",
    "    return bcolz.open(fname)[:]\n",
    "\n",
    "def save_resnet_vecs(vec_fname, img_name_fname, folder):\n",
    "    gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "    batches = gen.flow_from_directory(folder, \n",
    "                                      target_size=(224, 224), \n",
    "                                      batch_size=64, \n",
    "                                      class_mode=None,\n",
    "                                      shuffle=False)\n",
    "    vecs = resnet_model.predict_generator(batches, steps=(batches.samples + batches.batch_size) \n",
    "                                          // batches.batch_size)\n",
    "    save_array(vec_fname, vecs)\n",
    "    save_array(img_name_fname, batches.filenames)\n",
    "    \n",
    "def load_resnet_vecs(vec_fname, img_name_fname):\n",
    "    return load_array(img_name_fname), load_array(vec_fname)\n",
    "    \n",
    "save_resnet_vecs('train_vecs', 'train_img_name', '../data/train')\n",
    "save_resnet_vecs('test_vecs', 'test_img_name', '../data/test')\n",
    "save_resnet_vecs('valid_vecs', 'valid_img_name', '../data/valid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add two dense layers on top of Resnet50, finetune, and predict test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(vecs, labels, batch_size=64):\n",
    "    batch_vecs = []\n",
    "    batch_lables = []\n",
    "    rnd = random.Random()\n",
    "    rnd.seed(123)\n",
    "    while True:\n",
    "        index_shuf = [*range(len(vecs))]\n",
    "        rnd.shuffle(index_shuf)\n",
    "        for i in index_shuf:\n",
    "            batch_vecs.append(vecs[i])\n",
    "            batch_lables.append(labels[i])\n",
    "            if len(batch_vecs) == batch_size:\n",
    "                yield np.array(batch_vecs), np.array(batch_lables)\n",
    "                batch_vecs = []\n",
    "                batch_lables = []\n",
    "                \n",
    "                \n",
    "def test_generator(vecs, batch_size=64):\n",
    "    batch_vecs = []\n",
    "    for vec in vecs:\n",
    "        batch_vecs.append(vec)\n",
    "        if len(batch_vecs) == batch_size:\n",
    "            yield np.array(batch_vecs)\n",
    "            batch_vecs = []\n",
    "    yield np.array(batch_vecs)\n",
    "    \n",
    "                \n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))         \n",
    "        \n",
    "def train_predict_classes(classes, epochs=5):\n",
    "    inp = Input(shape=(2048, 1, 1))\n",
    "    x = Flatten()(inp)\n",
    "    x = Dense(200, activation='relu')(x)\n",
    "    x = Dense(len(classes), activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    fnames, vecs = load_resnet_vecs('train_vecs', 'train_img_name')\n",
    "    val_fnames, val_vecs = load_resnet_vecs('valid_vecs', 'valid_img_name')\n",
    "    df = pd.read_csv('../data/training_solutions_rev1.csv')[['GalaxyID'] + classes]\n",
    "    labels = np.array([df[df['GalaxyID'] == int(x[5:11])].drop('GalaxyID', axis=1).as_matrix()[0] \n",
    "                       for x in fnames])\n",
    "    val_labels = np.array([df[df['GalaxyID'] == int(x[5:11])].drop('GalaxyID', axis=1).as_matrix()[0] \n",
    "                       for x in val_fnames])\n",
    "    batch_size = 64\n",
    "    model.compile(optimizer='rmsprop', loss=root_mean_squared_error, metrics =[\"accuracy\"])\n",
    "    model.fit_generator(generator(vecs, labels, batch_size=batch_size), \n",
    "                        (len(fnames) + batch_size) // batch_size, \n",
    "                        validation_data=generator(vecs, labels, batch_size=batch_size), \n",
    "                        validation_steps=1, epochs=epochs)\n",
    "    \n",
    "    fnames, vecs = load_resnet_vecs('test_vecs', 'test_img_name')\n",
    "\n",
    "    batch_size = 64\n",
    "    predictions = model.predict_generator(test_generator(vecs, batch_size), \n",
    "                                      steps=(len(fnames) + batch_size) // batch_size, max_queue_size=1)\n",
    "    df = pd.DataFrame(predictions)\n",
    "    df = df.rename(columns={k: v for k, v in enumerate(classes)})\n",
    "\n",
    "    df['GalaxyID'] = [name[5:11] for name in fnames]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on all classes at once for 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "classes = train_predict_classes(['Class1.1', 'Class1.2', 'Class1.3', 'Class2.1',\n",
    "       'Class2.2', 'Class3.1', 'Class3.2', 'Class4.1', 'Class4.2',\n",
    "       'Class5.1', 'Class5.2', 'Class5.3', 'Class5.4', 'Class6.1',\n",
    "       'Class6.2', 'Class7.1', 'Class7.2', 'Class7.3', 'Class8.1',\n",
    "       'Class8.2', 'Class8.3', 'Class8.4', 'Class8.5', 'Class8.6',\n",
    "       'Class8.7', 'Class9.1', 'Class9.2', 'Class9.3', 'Class10.1',\n",
    "       'Class10.2', 'Class10.3', 'Class11.1', 'Class11.2', 'Class11.3',\n",
    "       'Class11.4', 'Class11.5', 'Class11.6'], 3)\n",
    "classes.to_csv('output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
